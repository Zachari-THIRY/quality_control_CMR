{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Monitoring for Brain Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Extract part of the function\n",
    "\n",
    "def downloadBrainData(relPath = \"./data/brain\", customUrl = None):\n",
    "\n",
    "    download_command = \"wget https://surfer.nmr.mgh.harvard.edu/ftp/dist/freesurfer/synthstrip/synthstrip_data_v1.2.tar.gz\"\n",
    "    if relPath is not None:\n",
    "        download_command += \" -P {}\".format(relPath)\n",
    "    \n",
    "    if not os.path.exists(os.path.join(relPath, \"synthstrip_data_v1.2.tar.gz\")):\n",
    "        !{download_command}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloadBrainData(relPath=\"./data/brain/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_command = \"tar -xf {REL_PATH}/synthstrip_data_v1.2.tar.gz -C {REL_PATH}/\".format(REL_PATH='./data/brain')\n",
    "!{extract_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prepatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from utils import generate_patient_info_brain, preprocess_brain, structure_dataset, find_segmentations, median_spacing_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentations_paths = find_segmentations(root_dir=\"data/brain/synthstrip_data_v1.2/\", keywords=[\"mask\"])\n",
    "structure_dataset(segmentation_paths=segmentations_paths, destination_folder=\"data/brain/structured\", fileName=\"mask.nii.gz\", delete=\"data/brain/synthstrip_data_v1.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_info = generate_patient_info_brain(folder=\"data/brain/structured/\", fileName=\"mask.nii.gz\")\n",
    "\n",
    "if not os.path.exists(\"data/brain/preprocessed\"):\n",
    "    os.makedirs(\"data/brain/preprocessed\")  \n",
    "np.save(os.path.join(\"data/brain/preprocessed/\", \"patient_info\"), patient_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_info = np.load(\"data/brain/preprocessed/patient_info.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing_target = median_spacing_target(\"data/brain/preprocessed\")\n",
    "\n",
    "if not os.path.exists(\"data/brain/preprocessed\"):\n",
    "    os.makedirs(\"data/brain/preprocessed\")\n",
    "    \n",
    "preprocess_brain(\n",
    "    range(0,444), patient_info, spacing_target,\n",
    "    \"data/brain/structured/\", \"data/brain/preprocessed\",\n",
    "    lambda folder, id: os.path.join(folder, 'patient{:03d}'.format(id)),\n",
    "    lambda : \"mask.nii.gz\",\n",
    "    skip=[9],\n",
    "    verbose=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torchvision\n",
    "from utils import AddPadding, CenterCrop, OneHot, ToTensor, MirrorTransform, SpatialTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = random.sample(range(0, 444),444)\n",
    "train_ids = ids[:266]\n",
    "val_ids = ids[266:355]\n",
    "test_ids = ids[355:444]\n",
    "\n",
    "saved_ids = {'train_ids': train_ids, 'val_ids': val_ids, 'test_ids': test_ids}\n",
    "np.save('./data/brain/saved_ids', saved_ids)\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    AddPadding((256,256)),\n",
    "    CenterCrop((256,256)),\n",
    "    OneHot(),\n",
    "    ToTensor()\n",
    "])\n",
    "transform_augmentation = torchvision.transforms.Compose([\n",
    "    MirrorTransform(),\n",
    "    SpatialTransform(patch_size=(256,256), angle_x=(-np.pi/6,np.pi/6), scale=(0.7,1.4), random_crop=True),\n",
    "    OneHot(),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed from train_ids\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    train_ids.remove(9)\n",
    "    print(\"Removed from train_ids\")\n",
    "except:\n",
    "    try: \n",
    "        test_ids.remove(9)\n",
    "        print(\"Removed from test_ids\")\n",
    "    except:\n",
    "        val_ids.remove(9)\n",
    "        print(\"Removed from val_ids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, importlib\n",
    "import CA\n",
    "importlib.reload(sys.modules['CA'])\n",
    "importlib.reload(sys.modules['utils'])\n",
    "\n",
    "import torch\n",
    "from CA import AE, plot_history, hyperparameter_tuning\n",
    "from utils import SYNDalaLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following cell allows the user to tune his own hyperparameters. In case you wanna try our hyperparameters first, just skip this cell.\n",
    "RUN = False\n",
    "#this is a list of possible values being tested for each hyperparameter.\n",
    "parameters = {\n",
    "    \"DA\": [True, False], #data augmentation\n",
    "    \"latent_size\": [100, 500], #size of the latent space of the autoencoder\n",
    "    \"BATCH_SIZE\": [8, 16, 4],\n",
    "    \"optimizer\": [torch.optim.Adam],\n",
    "    \"lr\": [2e-4, 1e-4, 1e-3],\n",
    "    \"weight_decay\": [1e-5],\n",
    "    \"tuning_epochs\": [5, 10], #number of epochs each configuration is run for\n",
    "    \"functions\": [[\"GDLoss\", \"MSELoss\"], [\"GDLoss\"], [\"BKGDLoss\", \"BKMSELoss\"]], #list of loss functions to be evaluated. BK stands for \"background\", which is a predominant and not compulsory class (it can lead to a dumb local minimum retrieving totally black images).\n",
    "    \"settling_epochs_BKGDLoss\": [10, 0], #during these epochs BK has half the weight of LV, RV and MYO in the evaluation of BKGDLoss\n",
    "    \"settling_epochs_BKMSELoss\": [10, 0], #during these epochs BK has half the weight of LV, RV and MYO in the evaluation of BKMSELoss\n",
    "}\n",
    "\n",
    "#this is a list of rules cutting out some useless combinations of hyperparameters from the tuning process.\n",
    "rules = [\n",
    "    '\"settling_epochs_BKGDLoss\" == 0 or \"BKGDLoss\" in \"functions\"',\n",
    "    '\"settling_epochs_BKMSELoss\" == 0 or \"BKMSELoss\" in \"functions\"',\n",
    "    '\"BKGDLoss\" not in \"functions\" or \"settling_epochs_BKGDLoss\" <= \"tuning_epochs\"',\n",
    "    '\"BKMSELoss\" not in \"functions\" or \"settling_epochs_BKMSELoss\" <= \"tuning_epochs\"',\n",
    "    #'\"BKGDLoss\" not in \"functions\" or \"settling_epochs_BKGDLoss\" >= \"settling_epochs_BKMSELoss\"'\n",
    "]\n",
    "if RUN: \n",
    "    optimal_parameters = hyperparameter_tuning(\n",
    "        parameters,\n",
    "        SYNDalaLoader(\"data/brain/preprocessed/\", patient_ids=train_ids, batch_size=None, transform=None),\n",
    "        SYNDalaLoader(\"data/brain/preprocessed/\", patient_ids=val_ids, batch_size=None, transform=None),\n",
    "        transform, transform_augmentation,\n",
    "        rules,\n",
    "        fast=True) #very important parameter. When False, all combinations are tested to return the one retrieving the maximum DSC. When True, the first combination avoiding dumb local minima is returned.\n",
    "\n",
    "\n",
    "    np.save(os.path.join(\"data/brain/preprocessed/\", \"optimal_parameters\"), optimal_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): LeakyReLU(negative_slope=0.2)\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2)\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): LeakyReLU(negative_slope=0.2)\n",
      "    (15): Dropout(p=0.5, inplace=False)\n",
      "    (16): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): LeakyReLU(negative_slope=0.2)\n",
      "    (19): Dropout(p=0.5, inplace=False)\n",
      "    (20): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): LeakyReLU(negative_slope=0.2)\n",
      "    (23): Dropout(p=0.5, inplace=False)\n",
      "    (24): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (25): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): LeakyReLU(negative_slope=0.2)\n",
      "    (27): Dropout(p=0.5, inplace=False)\n",
      "    (28): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (30): LeakyReLU(negative_slope=0.2)\n",
      "    (31): Dropout(p=0.5, inplace=False)\n",
      "    (32): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (34): LeakyReLU(negative_slope=0.2)\n",
      "    (35): Dropout(p=0.5, inplace=False)\n",
      "    (36): Conv2d(32, 100, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(100, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): ConvTranspose2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): LeakyReLU(negative_slope=0.2)\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): ConvTranspose2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2)\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): LeakyReLU(negative_slope=0.2)\n",
      "    (15): Dropout(p=0.5, inplace=False)\n",
      "    (16): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): LeakyReLU(negative_slope=0.2)\n",
      "    (19): Dropout(p=0.5, inplace=False)\n",
      "    (20): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): LeakyReLU(negative_slope=0.2)\n",
      "    (23): Dropout(p=0.5, inplace=False)\n",
      "    (24): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): LeakyReLU(negative_slope=0.2)\n",
      "    (27): Dropout(p=0.5, inplace=False)\n",
      "    (28): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (30): LeakyReLU(negative_slope=0.2)\n",
      "    (31): Dropout(p=0.5, inplace=False)\n",
      "    (32): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (34): LeakyReLU(negative_slope=0.2)\n",
      "    (35): Dropout(p=0.5, inplace=False)\n",
      "    (36): ConvTranspose2d(32, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (37): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "upload_your_own_parameters = False\n",
    "\n",
    "if upload_your_own_parameters:\n",
    "    optimal_parameters = np.load(os.path.join(\"data/brain/preprocessed\", \"optimal_parameters.npy\"), allow_pickle=True).item()\n",
    "else:\n",
    "    optimal_parameters = {\n",
    "        \"BATCH_SIZE\": 8,\n",
    "        \"DA\": False,\n",
    "        \"latent_size\": 100,\n",
    "        \"optimizer\": torch.optim.Adam,\n",
    "        \"lr\": 2e-4,\n",
    "        \"weight_decay\": 1e-5,\n",
    "        \"functions\": [\"BKGDLoss\", \"BKMSELoss\"],\n",
    "        \"settling_epochs_BKGDLoss\": 10,\n",
    "        \"settling_epochs_BKMSELoss\": 0\n",
    "    }\n",
    "    np.save(os.path.join(\"data/brain/preprocessed/\", \"optimal_parameters\"), optimal_parameters)\n",
    "\n",
    "assert optimal_parameters is not None, \"Be sure to continue with a working set of hyperparameters\"\n",
    "\n",
    "BATCH_SIZE = optimal_parameters[\"BATCH_SIZE\"]\n",
    "DA = optimal_parameters[\"DA\"]\n",
    "\n",
    "ae = AE(**optimal_parameters).to(device)\n",
    "\n",
    "ckpt = None\n",
    "if ckpt is not None:\n",
    "    ckpt = torch.load(ckpt)\n",
    "    ae.load_state_dict(ckpt[\"AE\"])\n",
    "    ae.optimizer.load_state_dict(ckpt[\"AE_optim\"])\n",
    "    start = ckpt[\"epoch\"]+1\n",
    "else:\n",
    "    start = 0\n",
    "\n",
    "print(ae)\n",
    "\n",
    "plot_history(\n",
    "    ae.training_routine(\n",
    "        range(start,100),\n",
    "        SYNDalaLoader(\"data/brain/preprocessed/\", patient_ids=train_ids, batch_size=BATCH_SIZE, transform=transform_augmentation if DA else transform),\n",
    "        SYNDalaLoader(\"data/brain/preprocessed/\", patient_ids=val_ids, batch_size=BATCH_SIZE, transform=transform),\n",
    "        \"data/brain/checkpoints/\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import nibabel as nib\n",
    "import torchvision\n",
    "\n",
    "from CA import AE\n",
    "from utils import postprocess_image_brain, evaluate_metrics, testing_brain, median_spacing_target\n",
    "from utils import display_image, display_difference, process_results, display_plots, SYNDalaLoader\n",
    "from utils import AddPadding, CenterCrop, OneHot, ToTensor, MirrorTransform, SpatialTransform\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    AddPadding((256,256)),\n",
    "    CenterCrop((256,256)),\n",
    "    OneHot(),\n",
    "    ToTensor()\n",
    "])\n",
    "transform_augmentation = torchvision.transforms.Compose([\n",
    "    MirrorTransform(),\n",
    "    SpatialTransform(patch_size=(256,256), angle_x=(-np.pi/6,np.pi/6), scale=(0.7,1.4), random_crop=True),\n",
    "    OneHot(),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen checkpoint is short_090_best.pth .\n",
      "###################################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(4, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): LeakyReLU(negative_slope=0.2)\n",
       "    (7): Dropout(p=0.5, inplace=False)\n",
       "    (8): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2)\n",
       "    (11): Dropout(p=0.5, inplace=False)\n",
       "    (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): LeakyReLU(negative_slope=0.2)\n",
       "    (15): Dropout(p=0.5, inplace=False)\n",
       "    (16): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): LeakyReLU(negative_slope=0.2)\n",
       "    (19): Dropout(p=0.5, inplace=False)\n",
       "    (20): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): LeakyReLU(negative_slope=0.2)\n",
       "    (23): Dropout(p=0.5, inplace=False)\n",
       "    (24): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (25): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): LeakyReLU(negative_slope=0.2)\n",
       "    (27): Dropout(p=0.5, inplace=False)\n",
       "    (28): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (30): LeakyReLU(negative_slope=0.2)\n",
       "    (31): Dropout(p=0.5, inplace=False)\n",
       "    (32): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (34): LeakyReLU(negative_slope=0.2)\n",
       "    (35): Dropout(p=0.5, inplace=False)\n",
       "    (36): Conv2d(32, 100, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose2d(100, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): ConvTranspose2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): LeakyReLU(negative_slope=0.2)\n",
       "    (7): Dropout(p=0.5, inplace=False)\n",
       "    (8): ConvTranspose2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2)\n",
       "    (11): Dropout(p=0.5, inplace=False)\n",
       "    (12): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): LeakyReLU(negative_slope=0.2)\n",
       "    (15): Dropout(p=0.5, inplace=False)\n",
       "    (16): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): LeakyReLU(negative_slope=0.2)\n",
       "    (19): Dropout(p=0.5, inplace=False)\n",
       "    (20): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): LeakyReLU(negative_slope=0.2)\n",
       "    (23): Dropout(p=0.5, inplace=False)\n",
       "    (24): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): LeakyReLU(negative_slope=0.2)\n",
       "    (27): Dropout(p=0.5, inplace=False)\n",
       "    (28): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (30): LeakyReLU(negative_slope=0.2)\n",
       "    (31): Dropout(p=0.5, inplace=False)\n",
       "    (32): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (34): LeakyReLU(negative_slope=0.2)\n",
       "    (35): Dropout(p=0.5, inplace=False)\n",
       "    (36): ConvTranspose2d(32, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (37): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_parameters = np.load(os.path.join(\"data/brain/preprocessed\", \"optimal_parameters.npy\"), allow_pickle=True).item()\n",
    "assert optimal_parameters is not None, \"Be sure to continue with a working set of hyperparameters\"\n",
    "\n",
    "BATCH_SIZE = optimal_parameters[\"BATCH_SIZE\"]\n",
    "DA = optimal_parameters[\"DA\"]\n",
    "\n",
    "ckpt = os.path.join(\"data/brain/checkpoints/\", sorted([file for file in os.listdir(\"data/brain/checkpoints\") if \"_best\" in file])[-1])\n",
    "print(\"Chosen checkpoint is {} .\".format(os.path.split(ckpt)[1]))\n",
    "print(\"###################################\")\n",
    "ckpt = torch.load(ckpt)\n",
    "\n",
    "ae = AE(**optimal_parameters).to(device)\n",
    "ae.load_state_dict(ckpt[\"AE\"])\n",
    "ae.optimizer.load_state_dict(ckpt[\"AE_optim\"])\n",
    "ae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_info = np.load(\"data/brain/preprocessed/patient_info.npy\", allow_pickle=True).item()\n",
    "current_spacing = median_spacing_target(\"data/brain/preprocessed/\", 2)\n",
    "test_ids = np.load('./data/brain/saved_ids.npy', allow_pickle=True).item().get('test_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = SYNDalaLoader(\"data/brain/preprocessed/\", test_ids, batch_size=BATCH_SIZE, transform=transform)\n",
    "\n",
    "results = testing_brain(ae=ae, test_loader=test_loader, patient_info=patient_info, folder_predictions=\"data/brain/structured/\", folder_out=\"data/brain/reconstructions\", current_spacing=current_spacing)\n",
    "#TODO: Rewrite a generic testing function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "## General"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import display_image, display_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = 236\n",
    "\n",
    "prediction = np.round(nib.load(\"data/brain/structured/patient{:03d}/mask.nii.gz\".format(patient_id)).get_fdata().transpose(2, 1, 0),2)\n",
    "reconstruction = np.round(nib.load(\"data/brain/reconstructions/patient{:03d}/mask.nii.gz\".format(patient_id)).get_fdata().transpose(2, 1, 0),2)\n",
    "\n",
    "mid_frame = prediction.shape[0]//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAAEEElEQVR4nO2dW7LUMAwFDcUWZ5WzST54UxNHkiVLVrq/gILY6hwrj7l4xgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8Ob1Ch/iS/gIZn4X/44c5WvkwZd4ffhVAGUFxIf/B1UFvC5/40zRHvCp5JhWUDMBu/I/iibgrn7PLBwpYDg6qChAtgCcFNTsARKc+sS5ApzukwsKkNflYaCgAAUOIagmQFvSsoJiVwFTOUvXg1oCrKdzQUEpAQtxNiuoJGBtORsVVGuCdoztsE4CPC7qhhSUSYDLbZ3hIGUE+KBfB1UEuL0C0R6oiADHV0DKQxUR4InOQA0BG98B/k8JAc71qw5XQsA79MOvOQVuhALyrxCan4DE9T9GAQEh9SsOmi4gm6YC5BFoKkBO4lUgtvtJLwR5CUju/r9IExBdv/T4WQKKnP/OTVCoOEnAjgDIxuibAKGBHAGbOoBkmBQB2zqgYKDOS2BIDGQI2HkJvB0rQUCZW4AxRoaAzfXfDbf7YSjh9M8fi/YKyEn/1MC3XbMY1Rb/T7YlILP6WQQ2JaDkyR9jOCXgr/I+yc6vfhKBRQGfa3vf/o3NRAkoUZyIawMLPeCc8meYBfQo334rfFj919M1Cjis/gk2AX3qtwloVL9JwIn1X87ZIODE+q9RXwZ7la9PQLf6tQLa1d/9tfg9OgH9AqAT0LB+lYCO9dMDFAJaBoAEICB7AtnIBfRsASQAAdkT2MTlBwNiAU1bwGMScMlDBFx/NPYQAdc8Q8Dk0+FnCJiAgOwJ7GD2M0KPEDDjCQKmPyf4BAFTHiBg/qOy/QXc/A9KsYDEPQ5CaZ+AuxPXXsAd3QXcrtzmAu47l1xA0y7YOwGCk9ZagCS0CgHHrQHRhFsnQIJGwGERkE23bwKEp0sl4LAIiGibgPrb6MQiDqtOQMM10DQB8jOlFHBIBI7aUDEZrYAjIqCZpDoBBxhQTZEloP4X5SOgm6AhAcUNKKdnWQLFDegw9YDKBrRzszXBugbUMzNeBaoa0M/LehmsaWDnl6xUNGCZk/1GqJ4B04wW7gSrGbDNp80uMtbTsbaPUBkD9jSu7iRVQ0Hm1+0VMLDUjJYfh/Nb4doM1t8HZBtYHN97N7ntrPp32lAxTcFy/pxeiWWtg/Vx/bbULLdfrAy/l6L7Q+AyouumqltD4CTceVfZfQq8Aue9re4mA37rzX9f4Q0KPNtNxMbKwQp8u23MztJxCtyvNWFba4c4CLjURu4t7u4g4lYjfHN1Pwsxd1pbdpd3kBB2n7lre/01B4G32VnfMPH+8GcXxD5kFPja3TGREP+EVUPARwXv144HzCoCxr8O9j1bFxLwR0H2e1YAAAAAAAAAAAAAAAAA6Md3nCVutccG/TQAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=256x256>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_image(prediction[mid_frame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAADrklEQVR4nO2dS24bMRAFmSBX1Cl9ySycAAlgi83+sJ9aVRsDhk2xi48cjiBx1gIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAB4GX50d2DP48/Pj5LWxQU8vvplqglpAV+Wv9bKdCAs4PvyP8mRICtgV/5KMqAqwFD/WhkOfoZbKMFYv/nvvkdTgL2usAHJKXBWVWwaCAo4HtSQAb0pcB7q0DSQS4CvGn8I9BLgwh8CNQHeStwGxATEr+uniAnw83C6k1oEg+PvWgnHJGA5/SkJCC8AngaEBCQsgI4mdASkXADOG1FZBNOuf6croU4Ckjg1KSIgcQN02JSGgNQN4FljEgKSN8BHzSkIuH8D8A8KAtI5uS8QENAaAAEBJfXbG20X0Dv+/QKq6je32y2gDKuB3nuB0vzb7gpaE1A7/22tdwroXv/WWq0Cyus3vUCfgAvjb3mJNgES+V99Au7Ub3iVJgEq49+0D7hY/nYz0JEAneFfLQKu1r99sfsCpMZ/rV+XX0+s/MF3g3/ZGb+bALnxv5wAwfoz9gHPy/ow/VUlz7cCYQGKo/o/zwUE1wD98nfE1oCXqP95JyMJeInyd4zfB+wICBgRgICAGfX7BQyp3y1gSv0sggjw/duYGUACfALmBIAEIMDzT4NmAAnwCJgUgHdIwPO3xN5AwHMQ0N2BbhwCRq2BJAAB3R0oZ/MhmXMBs5aAN0jAhvECdh8TGy9gBwK6O9ANAro7UIzkR2WlQEB3B2rZf3NsuIA9swUYvjp4LqDmgOc2RifAMlajBViYLMA0WR0CZi0CgxPwAt8eV8Aj4DXmgLGXYxNgHSWXgNeIgI2pCTCP0VAB9oz6BAyaA84EiBs46N7IKXAyPF4ByhE46tvABJyNjVuAbAQOO+ZPgKiBdz9W93hYAgIUI3Dep9CXp9U+LOIZktAUEMtAw/MFpAz4OhNcBKUMuIifICGyEHiHIn4Z1AiBuxcpZ4n1h6D5SVPtIQh0IOs0udYQRAYg7zi9NgU6zxvsURCcf7kHKjYoiK4/2SdK3lYQXn/zj9S8qiB+/ak4U/SegoTrb82hqpcUZOw/qk6VvaBA/dnj1QqStp+V5woLPD1gT+WbopW3CGltV58sXZOCRLX1R2sXKMiM1pWzxXMd5M6sW4ertz1QccfN0+XDEipW1dvH6wck1FxUep4zdK6h7JLa96ClAwmVGwrpp862v9kKAAAAAAAAAAAAAAAAADCF32SPWvLYfTgSAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=256x256>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_image(reconstruction[mid_frame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=256x256>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_difference(reconstruction[mid_frame], prediction[mid_frame])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 26 2021, 20:14:08) \n[GCC 9.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "26d733a98facd73c2412bf9332d44f71a6215b75d7e140b7f3987c8b99d7cfb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
