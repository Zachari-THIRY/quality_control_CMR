{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Monitoring for Liver Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to download your own Kaggle API credentials and place them in the root directory under the name \"kaggle.json\"\n",
    "kaggle_info = json.load(open(\"kaggle.json\"))\n",
    "\n",
    "KAGGLE_USERNAME = kaggle_info['username']\n",
    "KAGGLE_KEY = kaggle_info['key']\n",
    "\n",
    "# Exporting the required variables\n",
    "\n",
    "command = f\"export KAGGLE_USERNAME={KAGGLE_USERNAME} && export KAGGLE_KEY={KAGGLE_KEY}\"\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liver-tumor-segmentation.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('./data/liver'):\n",
    "    os.mkdir('./data/liver')\n",
    "command='kaggle datasets download -d andrewmvd/liver-tumor-segmentation -p ./data/liver'\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_command = \"unzip -q {REL_PATH}/{ZIP_NAME} -d {REL_PATH}/{FOLDER_NAME}\".format(REL_PATH='./data/liver', ZIP_NAME = \"liver-tumor-segmentation.zip\", FOLDER_NAME = \"liver-tumor-segmentation\")\n",
    "!{extract_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from utils import generate_patient_info_brain, preprocess_brain, structure_dataset, find_segmentations, median_spacing_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 segmentations were successfully retrieved.\n"
     ]
    }
   ],
   "source": [
    "segmentations_paths = find_segmentations(root_dir=\"data/liver/liver-tumor-segmentation/\", keywords=[\"segmentation\"])\n",
    "print(f\"{len(segmentations_paths)} segmentations were successfully retrieved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/liver/liver-tumor-segmentation/segmentations/segmentation-1.nii'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmentations_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete = [\n",
    "    \"data/liver/liver-tumor-segmentation/volume_pt1/\",\n",
    "    \"data/liver/liver-tumor-segmentation/volume_pt2/\",\n",
    "    \"data/liver/liver-tumor-segmentation/volume_pt3/\",\n",
    "    \"data/liver/liver-tumor-segmentation/volume_pt4/\",\n",
    "    \"data/liver/liver-tumor-segmentation/volume_pt5/\",\n",
    "    \"data/liver/liver-tumor-segmentation/\"\n",
    "    ]\n",
    "\n",
    "structure_dataset(segmentation_paths=segmentations_paths, destination_folder=\"data/liver/structured\", fileName=\"mask.nii.gz\", delete=delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_info = generate_patient_info_brain(folder=\"data/liver/structured/\", fileName=\"mask.nii.gz\", skip=[])\n",
    "\n",
    "if not os.path.exists(\"data/liver/preprocessed\"):\n",
    "    os.makedirs(\"data/liver/preprocessed\")  \n",
    "np.save(os.path.join(\"data/liver/preprocessed/\", \"patient_info\"), patient_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing patient 000\n",
      "Finished processing patient 010\n",
      "Finished processing patient 020\n",
      "Finished processing patient 030\n",
      "Finished processing patient 040\n",
      "Finished processing patient 050\n",
      "Finished processing patient 060\n",
      "Finished processing patient 070\n",
      "Finished processing patient 080\n",
      "Finished processing patient 090\n",
      "Finished processing patient 100\n",
      "Finished processing patient 110\n",
      "Finished processing patient 120\n",
      "Finished processing patient 130\n"
     ]
    }
   ],
   "source": [
    "spacing_target = median_spacing_target(\"data/liver/preprocessed\")\n",
    "\n",
    "if not os.path.exists(\"data/liver/preprocessed\"):\n",
    "    os.makedirs(\"data/liver/preprocessed\")\n",
    "    \n",
    "preprocess_brain(\n",
    "    range(0,131), patient_info, spacing_target,\n",
    "    \"data/liver/structured/\", \"data/liver/preprocessed\",\n",
    "    lambda folder, id: os.path.join(folder, 'patient{:03d}'.format(id)),\n",
    "    lambda : \"mask.nii.gz\",\n",
    "    skip=[],\n",
    "    verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shape': (512, 512, 771),\n",
       " 'crop': (slice(99, 355, None), slice(128, 346, None), slice(326, 494, None)),\n",
       " 'spacing': array([0.8  , 0.729, 0.729], dtype=float32),\n",
       " 'header': <nibabel.nifti1.Nifti1Header at 0x7f65b1b8d850>,\n",
       " 'affine': array([[-7.28999972e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "          1.91041504e+02],\n",
       "        [ 0.00000000e+00, -7.28999972e-01,  0.00000000e+00,\n",
       "          1.86354004e+02],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  8.00000012e-01,\n",
       "          1.39500000e+03],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          1.00000000e+00]]),\n",
       " 'processed_shape': array([168, 206, 242])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_info[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torchvision\n",
    "from utils import AddPadding, CenterCrop, OneHot, ToTensor, MirrorTransform, SpatialTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = random.sample(range(0, 131),131)\n",
    "train_ids = ids[:80]\n",
    "val_ids = ids[80:100]\n",
    "test_ids = ids[100:130]\n",
    "\n",
    "saved_ids = {'train_ids': train_ids, 'val_ids': val_ids, 'test_ids': test_ids}\n",
    "np.save('./data/liver/saved_ids', saved_ids)\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    AddPadding((256,256)),\n",
    "    CenterCrop((256,256)),\n",
    "    OneHot(),\n",
    "    ToTensor()\n",
    "])\n",
    "transform_augmentation = torchvision.transforms.Compose([\n",
    "    MirrorTransform(),\n",
    "    SpatialTransform(patch_size=(256,256), angle_x=(-np.pi/6,np.pi/6), scale=(0.7,1.4), random_crop=True),\n",
    "    OneHot(),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, importlib\n",
    "import CA\n",
    "importlib.reload(sys.modules['CA'])\n",
    "importlib.reload(sys.modules['utils'])\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from utils import AddPadding, CenterCrop, OneHot, ToTensor, MirrorTransform, SpatialTransform\n",
    "\n",
    "from CA import AE, plot_history, hyperparameter_tuning\n",
    "from utils import SYNDalaLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following cell allows the user to tune his own hyperparameters. In case you wanna try our hyperparameters first, just skip this cell.\n",
    "RUN = False\n",
    "#this is a list of possible values being tested for each hyperparameter.\n",
    "parameters = {\n",
    "    \"DA\": [True, False], #data augmentation\n",
    "    \"latent_size\": [100, 500], #size of the latent space of the autoencoder\n",
    "    \"BATCH_SIZE\": [8, 16, 4],\n",
    "    \"optimizer\": [torch.optim.Adam],\n",
    "    \"lr\": [2e-4, 1e-4, 1e-3],\n",
    "    \"weight_decay\": [1e-5],\n",
    "    \"tuning_epochs\": [5, 10], #number of epochs each configuration is run for\n",
    "    \"functions\": [[\"GDLoss\", \"MSELoss\"], [\"GDLoss\"], [\"BKGDLoss\", \"BKMSELoss\"]], #list of loss functions to be evaluated. BK stands for \"background\", which is a predominant and not compulsory class (it can lead to a dumb local minimum retrieving totally black images).\n",
    "    \"settling_epochs_BKGDLoss\": [10, 0], #during these epochs BK has half the weight of LV, RV and MYO in the evaluation of BKGDLoss\n",
    "    \"settling_epochs_BKMSELoss\": [10, 0], #during these epochs BK has half the weight of LV, RV and MYO in the evaluation of BKMSELoss\n",
    "}\n",
    "\n",
    "#this is a list of rules cutting out some useless combinations of hyperparameters from the tuning process.\n",
    "rules = [\n",
    "    '\"settling_epochs_BKGDLoss\" == 0 or \"BKGDLoss\" in \"functions\"',\n",
    "    '\"settling_epochs_BKMSELoss\" == 0 or \"BKMSELoss\" in \"functions\"',\n",
    "    '\"BKGDLoss\" not in \"functions\" or \"settling_epochs_BKGDLoss\" <= \"tuning_epochs\"',\n",
    "    '\"BKMSELoss\" not in \"functions\" or \"settling_epochs_BKMSELoss\" <= \"tuning_epochs\"',\n",
    "    #'\"BKGDLoss\" not in \"functions\" or \"settling_epochs_BKGDLoss\" >= \"settling_epochs_BKMSELoss\"'\n",
    "]\n",
    "if RUN: \n",
    "    optimal_parameters = hyperparameter_tuning(\n",
    "        parameters,\n",
    "        SYNDalaLoader(\"data/liver/preprocessed/\", patient_ids=train_ids, batch_size=None, transform=None),\n",
    "        SYNDalaLoader(\"data/liver/preprocessed/\", patient_ids=val_ids, batch_size=None, transform=None),\n",
    "        transform, transform_augmentation,\n",
    "        rules,\n",
    "        fast=True) #very important parameter. When False, all combinations are tested to return the one retrieving the maximum DSC. When True, the first combination avoiding dumb local minima is returned.\n",
    "\n",
    "\n",
    "    np.save(os.path.join(\"data/liver/preprocessed/\", \"optimal_parameters\"), optimal_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    AddPadding((256,256)),\n",
    "    CenterCrop((256,256)),\n",
    "    OneHot(),\n",
    "    ToTensor()\n",
    "])\n",
    "transform_augmentation = torchvision.transforms.Compose([\n",
    "    MirrorTransform(),\n",
    "    SpatialTransform(patch_size=(256,256), angle_x=(-np.pi/6,np.pi/6), scale=(0.7,1.4), random_crop=True),\n",
    "    OneHot(),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): LeakyReLU(negative_slope=0.2)\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2)\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): LeakyReLU(negative_slope=0.2)\n",
      "    (15): Dropout(p=0.5, inplace=False)\n",
      "    (16): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): LeakyReLU(negative_slope=0.2)\n",
      "    (19): Dropout(p=0.5, inplace=False)\n",
      "    (20): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): LeakyReLU(negative_slope=0.2)\n",
      "    (23): Dropout(p=0.5, inplace=False)\n",
      "    (24): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (25): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): LeakyReLU(negative_slope=0.2)\n",
      "    (27): Dropout(p=0.5, inplace=False)\n",
      "    (28): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (30): LeakyReLU(negative_slope=0.2)\n",
      "    (31): Dropout(p=0.5, inplace=False)\n",
      "    (32): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (34): LeakyReLU(negative_slope=0.2)\n",
      "    (35): Dropout(p=0.5, inplace=False)\n",
      "    (36): Conv2d(32, 100, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(100, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): ConvTranspose2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): LeakyReLU(negative_slope=0.2)\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): ConvTranspose2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2)\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): LeakyReLU(negative_slope=0.2)\n",
      "    (15): Dropout(p=0.5, inplace=False)\n",
      "    (16): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): LeakyReLU(negative_slope=0.2)\n",
      "    (19): Dropout(p=0.5, inplace=False)\n",
      "    (20): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): LeakyReLU(negative_slope=0.2)\n",
      "    (23): Dropout(p=0.5, inplace=False)\n",
      "    (24): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): LeakyReLU(negative_slope=0.2)\n",
      "    (27): Dropout(p=0.5, inplace=False)\n",
      "    (28): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (30): LeakyReLU(negative_slope=0.2)\n",
      "    (31): Dropout(p=0.5, inplace=False)\n",
      "    (32): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (34): LeakyReLU(negative_slope=0.2)\n",
      "    (35): Dropout(p=0.5, inplace=False)\n",
      "    (36): ConvTranspose2d(32, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (37): Softmax(dim=1)\n",
      "  )\n",
      ")\n",
      "\u001b[1mEpoch [11]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0344\t0.0147\t0.0491\t0.9781\t22.123\t0.9274\t24.852\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [12]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0308\t0.0131\t0.0439\t0.9795\t25.108\t0.9372\t24.862\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [13]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0321\t0.0135\t0.0456\t0.9799\t24.395\t0.9336\t26.223\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [14]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0309\t0.0128\t0.0437\t0.9812\t24.952\t0.9352\t25.682\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [15]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0340\t0.0142\t0.0481\t0.9788\t24.327\t0.9278\t25.110\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [16]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0344\t0.0141\t0.0486\t0.9784\t23.603\t0.9279\t24.505\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [17]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0307\t0.0124\t0.0431\t0.9816\t24.572\t0.9370\t23.568\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [18]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0303\t0.0123\t0.0426\t0.9821\t23.642\t0.9372\t23.279\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [19]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0329\t0.0136\t0.0465\t0.9801\t21.317\t0.9309\t23.554\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [20]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0308\t0.0123\t0.0431\t0.9817\t23.059\t0.9373\t22.816\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [21]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0303\t0.0122\t0.0425\t0.9823\t21.756\t0.9378\t22.713\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [22]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0278\t0.0113\t0.0391\t0.9844\t22.268\t0.9424\t22.441\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [23]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0311\t0.0124\t0.0435\t0.9823\t21.668\t0.9352\t22.856\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [24]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0287\t0.0115\t0.0402\t0.9838\t21.923\t0.9412\t21.951\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [25]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0272\t0.0110\t0.0383\t0.9851\t22.775\t0.9435\t22.125\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [26]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0315\t0.0130\t0.0445\t0.9812\t23.972\t0.9341\t23.999\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [27]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0291\t0.0117\t0.0407\t0.9825\t22.051\t0.9407\t21.963\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [28]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0324\t0.0131\t0.0455\t0.9809\t20.507\t0.9327\t22.461\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [29]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0309\t0.0124\t0.0433\t0.9817\t22.285\t0.9355\t22.338\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [30]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0272\t0.0110\t0.0382\t0.9850\t21.978\t0.9438\t21.578\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [31]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0279\t0.0114\t0.0393\t0.9843\t20.904\t0.9417\t21.725\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [32]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0254\t0.0103\t0.0358\t0.9864\t21.356\t0.9479\t21.808\t0.0000\tnan\t0.0000\tnan\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mEpoch [33]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0356\t0.0151\t0.0507\t0.9780\t21.990\t0.9240\t24.545\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [34]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0290\t0.0118\t0.0409\t0.9834\t20.814\t0.9396\t21.761\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [35]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0263\t0.0107\t0.0370\t0.9857\t20.679\t0.9457\t21.362\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [36]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0270\t0.0110\t0.0381\t0.9847\t20.723\t0.9442\t22.220\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [37]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0276\t0.0112\t0.0388\t0.9846\t21.003\t0.9434\t20.943\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [38]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0284\t0.0117\t0.0401\t0.9838\t22.579\t0.9410\t21.666\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [39]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0298\t0.0121\t0.0420\t0.9828\t21.886\t0.9372\t21.785\t0.0000\tnan\t0.0000\tnan\t\n",
      "\u001b[1mEpoch [40]\u001b[0m\n",
      "BKGDLo\tBKMSEL\tTotal\tBK_dc\tBK_hd\tRV_dc\tRV_hd\tMYO_dc\tMYO_hd\tLV_dc\tLV_hd\t\n",
      "0.0308\t0.0127\t0.0435\t0.9814\t20.818\t0.9355\t22.711\t0.0000\tnan\t0.0000\tnan\t\n"
     ]
    }
   ],
   "source": [
    "upload_your_own_parameters = False\n",
    "\n",
    "if upload_your_own_parameters:\n",
    "    optimal_parameters = np.load(os.path.join(\"data/liver/preprocessed\", \"optimal_parameters.npy\"), allow_pickle=True).item()\n",
    "else:\n",
    "    optimal_parameters = {\n",
    "        \"BATCH_SIZE\": 8,\n",
    "        \"DA\": False,\n",
    "        \"latent_size\": 100,\n",
    "        \"optimizer\": torch.optim.Adam,\n",
    "        \"lr\": 2e-4,\n",
    "        \"weight_decay\": 1e-5,\n",
    "        \"functions\": [\"BKGDLoss\", \"BKMSELoss\"],\n",
    "        \"settling_epochs_BKGDLoss\": 10,\n",
    "        \"settling_epochs_BKMSELoss\": 0\n",
    "    }\n",
    "    np.save(os.path.join(\"data/liver/preprocessed/\", \"optimal_parameters\"), optimal_parameters)\n",
    "\n",
    "assert optimal_parameters is not None, \"Be sure to continue with a working set of hyperparameters\"\n",
    "\n",
    "BATCH_SIZE = optimal_parameters[\"BATCH_SIZE\"]\n",
    "DA = optimal_parameters[\"DA\"]\n",
    "\n",
    "train_ids = np.load('./data/liver/saved_ids.npy', allow_pickle=True).item().get('train_ids')\n",
    "val_ids = np.load('./data/liver/saved_ids.npy', allow_pickle=True).item().get('val_ids')\n",
    "ae = AE(**optimal_parameters).to(device)\n",
    "\n",
    "ckpt = \"data/liver/checkpoints/010_best.pth\"\n",
    "if ckpt is not None:\n",
    "    ckpt = torch.load(ckpt)\n",
    "    ae.load_state_dict(ckpt[\"AE\"])\n",
    "    ae.optimizer.load_state_dict(ckpt[\"AE_optim\"])\n",
    "    start = ckpt[\"epoch\"]+1\n",
    "else:\n",
    "    start = 0\n",
    "\n",
    "print(ae)\n",
    "\n",
    "plot_history(\n",
    "    ae.training_routine(\n",
    "        range(start,100),\n",
    "        SYNDalaLoader(\"data/liver/preprocessed/\", patient_ids=train_ids, batch_size=BATCH_SIZE, transform=transform_augmentation if DA else transform),\n",
    "        SYNDalaLoader(\"data/liver/preprocessed/\", patient_ids=val_ids, batch_size=BATCH_SIZE, transform=transform),\n",
    "        \"data/liver/checkpoints/\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiry/quality_control_CMR/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import nibabel as nib\n",
    "import torchvision\n",
    "import tqdm\n",
    "\n",
    "from CA import AE\n",
    "from utils import postprocess_image_brain, evaluate_metrics, testing_brain, median_spacing_target\n",
    "from utils import display_image, display_difference, process_results, display_plots, SYNDalaLoader\n",
    "from utils import AddPadding, CenterCrop, OneHot, ToTensor, MirrorTransform, SpatialTransform\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    AddPadding((256,256)),\n",
    "    CenterCrop((256,256)),\n",
    "    OneHot(),\n",
    "    ToTensor()\n",
    "])\n",
    "transform_augmentation = torchvision.transforms.Compose([\n",
    "    MirrorTransform(),\n",
    "    SpatialTransform(patch_size=(256,256), angle_x=(-np.pi/6,np.pi/6), scale=(0.7,1.4), random_crop=True),\n",
    "    OneHot(),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen checkpoint is 099_best.pth .\n",
      "###################################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(4, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): LeakyReLU(negative_slope=0.2)\n",
       "    (7): Dropout(p=0.5, inplace=False)\n",
       "    (8): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2)\n",
       "    (11): Dropout(p=0.5, inplace=False)\n",
       "    (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): LeakyReLU(negative_slope=0.2)\n",
       "    (15): Dropout(p=0.5, inplace=False)\n",
       "    (16): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): LeakyReLU(negative_slope=0.2)\n",
       "    (19): Dropout(p=0.5, inplace=False)\n",
       "    (20): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): LeakyReLU(negative_slope=0.2)\n",
       "    (23): Dropout(p=0.5, inplace=False)\n",
       "    (24): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (25): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): LeakyReLU(negative_slope=0.2)\n",
       "    (27): Dropout(p=0.5, inplace=False)\n",
       "    (28): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (30): LeakyReLU(negative_slope=0.2)\n",
       "    (31): Dropout(p=0.5, inplace=False)\n",
       "    (32): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (34): LeakyReLU(negative_slope=0.2)\n",
       "    (35): Dropout(p=0.5, inplace=False)\n",
       "    (36): Conv2d(32, 100, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose2d(100, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): ConvTranspose2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): LeakyReLU(negative_slope=0.2)\n",
       "    (7): Dropout(p=0.5, inplace=False)\n",
       "    (8): ConvTranspose2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2)\n",
       "    (11): Dropout(p=0.5, inplace=False)\n",
       "    (12): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): LeakyReLU(negative_slope=0.2)\n",
       "    (15): Dropout(p=0.5, inplace=False)\n",
       "    (16): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): LeakyReLU(negative_slope=0.2)\n",
       "    (19): Dropout(p=0.5, inplace=False)\n",
       "    (20): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): LeakyReLU(negative_slope=0.2)\n",
       "    (23): Dropout(p=0.5, inplace=False)\n",
       "    (24): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): LeakyReLU(negative_slope=0.2)\n",
       "    (27): Dropout(p=0.5, inplace=False)\n",
       "    (28): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (30): LeakyReLU(negative_slope=0.2)\n",
       "    (31): Dropout(p=0.5, inplace=False)\n",
       "    (32): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (34): LeakyReLU(negative_slope=0.2)\n",
       "    (35): Dropout(p=0.5, inplace=False)\n",
       "    (36): ConvTranspose2d(32, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (37): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_parameters = np.load(os.path.join(\"data/liver/preprocessed\", \"optimal_parameters.npy\"), allow_pickle=True).item()\n",
    "assert optimal_parameters is not None, \"Be sure to continue with a working set of hyperparameters\"\n",
    "\n",
    "BATCH_SIZE = optimal_parameters[\"BATCH_SIZE\"]\n",
    "DA = optimal_parameters[\"DA\"]\n",
    "\n",
    "ckpt = os.path.join(\"data/liver/checkpoints/\", sorted([file for file in os.listdir(\"data/liver/checkpoints\") if \"_best\" in file])[-1])\n",
    "print(\"Chosen checkpoint is {} .\".format(os.path.split(ckpt)[1]))\n",
    "print(\"###################################\")\n",
    "ckpt = torch.load(ckpt)\n",
    "\n",
    "ae = AE(**optimal_parameters).to(device)\n",
    "ae.load_state_dict(ckpt[\"AE\"])\n",
    "ae.optimizer.load_state_dict(ckpt[\"AE_optim\"])\n",
    "ae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_info = np.load(\"data/liver/preprocessed/patient_info.npy\", allow_pickle=True).item()\n",
    "current_spacing = median_spacing_target(\"data/liver/preprocessed/\", 2)\n",
    "test_ids = np.load('./data/liver/saved_ids.npy', allow_pickle=True).item().get('test_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = SYNDalaLoader(\"data/liver/preprocessed/\", test_ids, batch_size=BATCH_SIZE, transform=transform)\n",
    "\n",
    "results = testing_brain(ae=ae, test_loader=test_loader, patient_info=patient_info, folder_predictions=\"data/liver/structured/\", folder_out=\"data/liver/reconstructions\", current_spacing=current_spacing)\n",
    "#TODO: Rewrite a generic testing function\n",
    "np.save(os.path.join(\"data/liver/postprocessed/\", \"results\"), results)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "## General"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import display_image, display_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m patient_id \u001b[39m=\u001b[39m \u001b[39m106\u001b[39m\n\u001b[0;32m----> 3\u001b[0m prediction \u001b[39m=\u001b[39m nib\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mdata/liver/structured/patient\u001b[39m\u001b[39m{:03d}\u001b[39;00m\u001b[39m/mask.nii.gz\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(patient_id))\u001b[39m.\u001b[39mget_fdata()[:,:,:,\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m reconstruction \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mround(nib\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mdata/liver/reconstructions/patient\u001b[39m\u001b[39m{:03d}\u001b[39;00m\u001b[39m/mask.nii.gz\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(patient_id))\u001b[39m.\u001b[39mget_fdata()\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m),\u001b[39m2\u001b[39m)\n\u001b[1;32m      6\u001b[0m mid_frame \u001b[39m=\u001b[39m prediction\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nib' is not defined"
     ]
    }
   ],
   "source": [
    "patient_id = 106\n",
    "\n",
    "prediction = nib.load(\"data/liver/structured/patient{:03d}/mask.nii.gz\".format(patient_id)).get_fdata()[:,:,:,0].transpose(2, 1, 0)\n",
    "reconstruction = np.round(nib.load(\"data/liver/reconstructions/patient{:03d}/mask.nii.gz\".format(patient_id)).get_fdata().transpose(2, 1, 0),2)\n",
    "\n",
    "mid_frame = prediction.shape[0]//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAABnElEQVR4nO2bQW5DMQhE3apXzCl7yS5aKVIlw4BhPj+Z2SRRhHkM2It8Zy1JkiTpYn0k4x5/r9/Pt7+fKACP/VdhhASAkT6BEAVwsscJYgBA+ihCBABMH0PAAQLpIwgoQDA9TvDZlR8NwQAS+dEgqAWp/GtBbfhqTA8JnIGkAHYf4MgAP9gFOGyAG+4BHA+At0DvDAByAAp2gLOEDVCyA+1FZreg6Agyl7EAeo9AAKBMViWTZ4DSAZIDRi2DW8DpAMuBfTVbAJIBk2dAAAKo1XZTvY0DAggDsE7iuQ4kfvOsBaCJBbB19G0cEIAA4gCsk4jkwL6cPQDJAo4DRjFzh/ClAKxxmuwAZxswHDArsQAoFkyegSoL7FVmOyAAAVwPQDiK+h1wihjegusB+odgugPn8jz0ANp7ML8F3RZ0O+Dyz2/BWQ/84Bs40DyGvQ4A7AhA2gIkEHKgswl3GMKVtQCKAh3IEGAxaAviBLV3SuME1bdq14o9yIJ5W25WR+xquFse69btbtd7EIz/F/zXE4b2sFGSJEmSJOml9APPmjBnA+R5DAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_image(prediction[mid_frame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAABlklEQVR4nO2aS24DMQxDp71jTjmX7L5AJFK/yBNybcfPJMcIDF+XJEmSJBXpFZz3k1vv/r/yPQFgb5ZECAC4ZlMILAAWNYHAAeBNgxF+e9bHhxIOkB8a6AEOwH/oEAIcQeCggaagANGDrgwgJIQa60B4/34NWh1AyCGARAHcqQhAWwFBgNz63uzmDvgEPkBrABMOODtwAZoNmHDA3oMH0G2AB1CzvvUrExGYsgHaA1jvQJUMJ02AgQS+JYLPA7wPc7UDEx3c7YAABCCA9QD0pWQxwF1CkIuggODoDly4Be/HrXYA2V66BasdeBCAEZQJUHLUOUo4UPOXMRtBmsK+qi36X2xFaTswUIKv+AxNHx2A/gzWR9BuwYAD9h7WR9CewX4H0nIsPMCB5hK0O+DxHxCBAJpbeIIDKbn2IQCtGTw+gocAxEvw8Tckvg4BiGYAzDvEgaAFyCzUgbbTsDOC2ud8vAXYDNwBlgAcT0TAEaCjmQ4wBPBY8l0xemuFs9IvqxEExqrQ43YLYuJ2VZIkSZIkSZJK9Qc8VTBfNnncxwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_image(reconstruction[mid_frame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAACwUlEQVR4nO3dYW6DMAyG4W7aAXoY7n8CDtMj7AcSQkAhUNufnb6P9mvqRJQvdmjXpo8HAAAAAABAmB/1AJq8hmH3989xPH7A9pHZpA7gdFoPPMdx++cJY0gawGrudidufkz7tE5/kiqGXAEs591vml7DkCeDLAG0LHnzK2aIQR9AzKo/uLo2BmUA2qlfjUQ1AE0A8Q3nlCoDQQB5Fv6KJIPoANLOvspv5MWqzP4nTwCv+ou5TMKmn0RoBVSx+zKGk4g9oErnWYnZk90rILKf2oqpA98Aqrf+gAzi9oBysx/DMYCirX/Fuwi8Aqjb+rdcM4hoQXWXfwCXAHpa/hO/IrAPoPqdTzCeCbdyKgLfAFj+p4xfiujj1jMSLega8y7kFQDLv5FlAP3dfQagBV1jfi9kFgDb7z1UgBgBiBHAZbYNlgDE7ANgB76ECrjD8E6UAMQIQIwAxAhAzCYAXoa7jQoQswmAe//b6lVAZ+2uWADT7MszMKz4YgHM5BlYqRpANwoHoCqCpP+SxD3u7w111cFOQAWImQXAc7F7qIBrzD+7ahlA+xF6mBWrAHmjMx9AsQBW5Hl8LuiwDkPCSfc4vICPqV7gkX3tFtQB4wA6aMrvlPmY6qyzLuS0tnhroljocTXYcgmAImjHXZAYR5aJUQFiBHCu5IlZaBQRANvAAc6KONHPsZV1uS4mAhAjADHHAJaVyz78DhUgRgBHAk6wjzs1kS60iwoQcw+gm2dkTqiAIz18hclSrW0gZrQcX/9WzLBpQWJBAZR753rYOKmAfWFtU/AtSlWKIEZoBZDBFi1orf/F8RqG6Uc9kB05R2UvZwbZxuMrWwZ5RhInTwYZxiAjj+GrZ38miSHD1Ed8o3a7gG/hmy+R5CXCXAHMXNdmkqmfJA1gyTCMVFM/KRBAi+OQEs47AAAAAAAAAAAA8CX+AZFFCQaJvtwSAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_difference(reconstruction[mid_frame], prediction[mid_frame])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "26d733a98facd73c2412bf9332d44f71a6215b75d7e140b7f3987c8b99d7cfb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
